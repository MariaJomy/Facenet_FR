{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  STEP 1:Getting Embeddings.\n",
    "\n",
    "## It detects the face and returns probability with which the face is detected.\n",
    "## It returns the 512 embeddings vector,the most crucial for face recognition .\n",
    "## MTCNN is used for face detection,and for finding the bounding box,(which is later used for drawing box around the face).\n",
    "## Inception Resnet V1 model (defined here as resnet) is used for finding the embeddings vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essential imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1,extract_face #cloned from facenet pytorch model from github into \"facenet_pytorch\"\n",
    "from PIL import Image,ImageDraw   # A PIL image is sent to MTCNN\n",
    "\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\"\"\" while operating in webcam,the fps(frames per second) was really low,hence we include some moe functions to improve fps.\n",
    "\"\"\"\n",
    "from Web.imutils.video import WebcamVideoStream\n",
    "from Web.imutils.video import FPS\n",
    "import imutils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Essential Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cos_sim(): Returns the dot product of the vectors in the function.\n",
    "### uses numpy funcions linalg.norm() and dot()\n",
    "\n",
    "\n",
    "#### numpy.linalg.norm(x, ord=None, axis=None, keepdims=False)[source]\n",
    "\n",
    "    Matrix or vector norm.\n",
    "\n",
    "    This function is able to return one of eight different matrix norms, or one of an infinite number of vector norms (described below), depending on the value of the ord parameter.\n",
    "    Parameters:\t\n",
    "\n",
    "    x : array_like\n",
    "\n",
    "        Input array. If axis is None, x must be 1-D or 2-D.\n",
    "    ord : {non-zero int, inf, -inf, ‘fro’, ‘nuc’}, optional\n",
    "\n",
    "        Order of the norm (see table under Notes). inf means numpy’s inf object.\n",
    "    axis : {int, 2-tuple of ints, None}, optional\n",
    "\n",
    "        If axis is an integer, it specifies the axis of x along which to compute the vector norms. If axis is a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of these matrices are computed. If axis is None then either a vector norm (when x is 1-D) or a matrix norm (when x is 2-D) is returned.\n",
    "\n",
    "    keepdims : bool, optional\n",
    "\n",
    "        If this is set to True, the axes which are normed over are left in the result as dimensions with size one. With this option the result will broadcast correctly against the original x.\n",
    "\n",
    "        \n",
    "\n",
    "    Returns:\t\n",
    "\n",
    "    n : float or ndarray\n",
    "\n",
    "        Norm of the matrix or vector(s).\n",
    "        \n",
    "        \n",
    "####   numpy.dot(a, b, out=None)¶\n",
    "\n",
    "    Dot product of two arrays. Specifically,\n",
    "\n",
    "        If both a and b are 1-D arrays, it is inner product of vectors (without complex conjugation).\n",
    "\n",
    "        If both a and b are 2-D arrays, it is matrix multiplication, but using matmul or a @ b is preferred.\n",
    "\n",
    "        If either a or b is 0-D (scalar), it is equivalent to multiply and using numpy.multiply(a, b) or a * b is preferred.\n",
    "\n",
    "        If a is an N-D array and b is a 1-D array, it is a sum product over the last axis of a and b.\n",
    "\n",
    "        If a is an N-D array and b is an M-D array (where M>=2), it is a sum product over the last axis of a and the second-to-last axis of b:\n",
    "\n",
    "        dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n",
    "\n",
    "    Parameters:\t\n",
    "\n",
    "    a : array_like\n",
    "\n",
    "        First argument.\n",
    "    b : array_like\n",
    "\n",
    "        Second argument.\n",
    "    out : ndarray, optional\n",
    "\n",
    "        Output argument. This must have the exact kind that would be returned if it was not used. In particular, it must have the right type, must be C-contiguous, and its dtype must be the dtype that would be returned for dot(a,b). This is a performance feature. Therefore, if these conditions are not met, an exception is raised, instead of attempting to be flexible.\n",
    "\n",
    "    Returns\n",
    "\n",
    "    output : ndarray\n",
    "\n",
    "        Returns the dot product of a and b. If a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. If out is given, then it is returned.\n",
    "\n",
    "    Raises:\t\n",
    "\n",
    "    ValueError\n",
    "\n",
    "        If the last dimension of a is not the same size as the second-to-last dimension of b.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    \"\"\"Takes 2 vectors a, b and returns the cosine similarity according \n",
    "    to the definition of the dot product\"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cos(): cos_sim returns real numbers,where negative numbers have different interpretations.So we use this function to return only positive values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(a,b):\n",
    "    minx = -1 \n",
    "    maxx = 1\n",
    "    return (cos_sim(a,b)- minx)/(maxx-minx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify(): The key function in face recognition.\n",
    "#### It takes the embedding vector of the test image and compares it with the embedding vector of the existing dataset.\n",
    "#### The chosen threshold is 0.81(by trial and error and by checking accuracy(which is seen later)).\n",
    "#### If the image is not recognised,nothing is mentioned,but still if face is detected,it draws the bounding box around it.\n",
    "####  we print the names(also its mentioned in the image).\n",
    "\n",
    "Here,boxes is a numpy nd array,obtained from mtcnn.detect(),containing the coordinates of bounding boxes of thedetected faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def verify(embedding):\n",
    "    font = cv2.FONT_HERSHEY_DUPLEX\n",
    "    for i,k in enumerate(embeddings):\n",
    "        for j,l in enumerate(embedding):\n",
    "            dist =cos(k,l)\n",
    "    \n",
    "        #print(dist)\n",
    "    # Chosen threshold is 0.81  \n",
    "            if dist > 0.81:\n",
    "                text= names[i]\n",
    "                left=boxes[j][0].astype(int)\n",
    "                right=boxes[j][2].astype(int)\n",
    "                top=boxes[j][1].astype(int)\n",
    "                bottom=boxes[j][3].astype(int)\n",
    "                n=int((bottom-top)/6)\n",
    "                m=int((right-left)/10)\n",
    "                cv2.rectangle(ima, (left, bottom-n), (right, bottom), (0, 0,0), cv2.FILLED)\n",
    "               \n",
    "                cv2.putText(ima, text, (left +m, bottom - m), font, 1.0, (255, 255, 255), 1)\n",
    "                #cv2.putText(ima, text,(boxes[j][0].astype(int),boxes[j][3].astype(int)), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 1, (0,255,0), 2)\n",
    "                print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essential definitions\n",
    "\n",
    "### Since we use a pretrained model(in VGGFACE2 database),we require only cpu.So we define the device to be 'cpu'.\n",
    "### The pretrained model Inception Resnet V1 is called as resnet.\n",
    "### An instance of class MTCNN (as mtcnn) ,the parameters can be changed (refer facenet_pytorch repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu') # cpu ,since is already pre-trained and doesnt require gpu.\n",
    "print('Running on device: {}'.format(device))\n",
    "\n",
    "# Define MTCNN module\n",
    "\n",
    "# Note that, since MTCNN is a collection of neural nets and other code, the\n",
    "# device must be passed in the following way to enable copying of objects when\n",
    "# needed internally.\n",
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, prewhiten=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "\n",
    "# Define Inception Resnet V1 module\n",
    "\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detected with probability: 0.999996\n",
      "Face detected with probability: 0.999567\n",
      "Face detected with probability: 0.999999\n",
      "Face detected with probability: 0.999994\n",
      "Face detected with probability: 0.999403\n",
      "Face detected with probability: 0.995384\n",
      "Face detected with probability: 0.999852\n",
      "Face detected with probability: 0.999999\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 0.999992\n",
      "Face detected with probability: 0.999990\n",
      "Face detected with probability: 0.999998\n",
      "Face detected with probability: 0.999984\n",
      "Face detected with probability: 0.999992\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 0.999983\n",
      "Face detected with probability: 0.999997\n",
      "Face detected with probability: 0.999850\n",
      "Face detected with probability: 0.999953\n",
      "Face detected with probability: 0.999628\n",
      "Face detected with probability: 0.999971\n",
      "Face detected with probability: 0.999980\n",
      "Face detected with probability: 0.999989\n",
      "Face detected with probability: 0.999974\n",
      "Face detected with probability: 0.999998\n",
      "Face detected with probability: 0.999985\n",
      "Face detected with probability: 0.999998\n",
      "             amma      amma      amma   chettan   chettan   chettan   chettan  \\\n",
      "amma     1.000000  0.778833  0.780455  0.768470  0.724474  0.717318  0.757811   \n",
      "amma     0.778833  1.000000  0.927556  0.580799  0.570971  0.571074  0.576643   \n",
      "amma     0.780455  0.927556  1.000000  0.608711  0.563527  0.540085  0.601119   \n",
      "chettan  0.768470  0.580799  0.608711  1.000000  0.750521  0.757921  0.813978   \n",
      "chettan  0.724474  0.570971  0.563527  0.750521  1.000000  0.885134  0.763474   \n",
      "chettan  0.717318  0.571074  0.540085  0.757921  0.885134  1.000000  0.856185   \n",
      "chettan  0.757811  0.576643  0.601119  0.813978  0.763474  0.856185  1.000000   \n",
      "chettan  0.719022  0.549769  0.593742  0.912973  0.777631  0.779933  0.833342   \n",
      "maria    0.786292  0.733425  0.776885  0.678683  0.706734  0.653379  0.669644   \n",
      "maria    0.804600  0.761517  0.836507  0.701711  0.617579  0.585153  0.658814   \n",
      "maria    0.819846  0.736180  0.766626  0.685732  0.615996  0.586891  0.609657   \n",
      "maria    0.810948  0.737494  0.767541  0.723660  0.616611  0.561261  0.626086   \n",
      "maria    0.789091  0.726816  0.768183  0.694137  0.723808  0.682751  0.704892   \n",
      "maria    0.822204  0.745293  0.776843  0.747682  0.639792  0.574295  0.646419   \n",
      "maria    0.811932  0.805020  0.771518  0.601229  0.625202  0.680811  0.633393   \n",
      "maria    0.719932  0.778106  0.795143  0.560301  0.513352  0.492579  0.481254   \n",
      "miffi    0.800645  0.784214  0.785252  0.679460  0.602532  0.534935  0.557410   \n",
      "miffi    0.809507  0.780511  0.780594  0.705019  0.593853  0.541219  0.560635   \n",
      "miffi    0.778999  0.746264  0.766172  0.647820  0.616454  0.572078  0.629324   \n",
      "par_ma   0.745894  0.643027  0.655392  0.659552  0.623433  0.558814  0.597760   \n",
      "par_ma   0.747174  0.608904  0.675947  0.614749  0.606508  0.618154  0.684258   \n",
      "par_ma   0.703868  0.651945  0.677046  0.684621  0.595641  0.601479  0.641961   \n",
      "parvati  0.657668  0.618613  0.653211  0.579749  0.559973  0.562913  0.619295   \n",
      "parvati  0.614196  0.577707  0.620976  0.544774  0.507564  0.508693  0.517767   \n",
      "parvati  0.556115  0.527392  0.562517  0.456133  0.502300  0.538208  0.563242   \n",
      "parvati  0.669395  0.615461  0.629953  0.558116  0.541869  0.541751  0.579312   \n",
      "parvati  0.761215  0.638949  0.681467  0.596601  0.540973  0.599481  0.639134   \n",
      "parvati  0.696547  0.676239  0.717956  0.581448  0.530982  0.578200  0.601648   \n",
      "\n",
      "          chettan     maria     maria  ...     miffi    par_ma    par_ma  \\\n",
      "amma     0.719022  0.786292  0.804600  ...  0.778999  0.745894  0.747174   \n",
      "amma     0.549769  0.733425  0.761517  ...  0.746264  0.643027  0.608904   \n",
      "amma     0.593742  0.776885  0.836507  ...  0.766172  0.655392  0.675947   \n",
      "chettan  0.912973  0.678683  0.701711  ...  0.647820  0.659552  0.614749   \n",
      "chettan  0.777631  0.706734  0.617579  ...  0.616454  0.623433  0.606508   \n",
      "chettan  0.779933  0.653379  0.585153  ...  0.572078  0.558814  0.618154   \n",
      "chettan  0.833342  0.669644  0.658814  ...  0.629324  0.597760  0.684258   \n",
      "chettan  1.000000  0.675709  0.661808  ...  0.656717  0.672229  0.705271   \n",
      "maria    0.675709  1.000000  0.887574  ...  0.829827  0.698912  0.706964   \n",
      "maria    0.661808  0.887574  1.000000  ...  0.820508  0.634389  0.682887   \n",
      "maria    0.602740  0.848586  0.842691  ...  0.777590  0.636840  0.631413   \n",
      "maria    0.638725  0.907612  0.910855  ...  0.827947  0.694899  0.666310   \n",
      "maria    0.695528  0.989672  0.881156  ...  0.830197  0.704899  0.712669   \n",
      "maria    0.672561  0.900090  0.922264  ...  0.832359  0.709598  0.687005   \n",
      "maria    0.598629  0.834963  0.836183  ...  0.752062  0.626904  0.667465   \n",
      "maria    0.451908  0.769119  0.845548  ...  0.761301  0.564352  0.540759   \n",
      "miffi    0.568352  0.753635  0.803726  ...  0.858019  0.683016  0.631640   \n",
      "miffi    0.591295  0.741477  0.794796  ...  0.846194  0.688446  0.638685   \n",
      "miffi    0.656717  0.829827  0.820508  ...  1.000000  0.752099  0.731872   \n",
      "par_ma   0.672229  0.698912  0.634389  ...  0.752099  1.000000  0.853229   \n",
      "par_ma   0.705271  0.706964  0.682887  ...  0.731872  0.853229  1.000000   \n",
      "par_ma   0.689826  0.633724  0.659060  ...  0.657021  0.765124  0.817696   \n",
      "parvati  0.580418  0.613324  0.732314  ...  0.663699  0.600785  0.657099   \n",
      "parvati  0.516861  0.603061  0.711924  ...  0.686509  0.598265  0.602580   \n",
      "parvati  0.524843  0.597333  0.603153  ...  0.588801  0.642332  0.712907   \n",
      "parvati  0.529911  0.584443  0.680370  ...  0.681916  0.627394  0.621790   \n",
      "parvati  0.566635  0.614297  0.763351  ...  0.662461  0.662506  0.671283   \n",
      "parvati  0.566890  0.616703  0.740925  ...  0.708664  0.642547  0.663317   \n",
      "\n",
      "           par_ma   parvati   parvati   parvati   parvati   parvati   parvati  \n",
      "amma     0.703868  0.657668  0.614196  0.556115  0.669395  0.761215  0.696547  \n",
      "amma     0.651945  0.618613  0.577707  0.527392  0.615461  0.638949  0.676239  \n",
      "amma     0.677046  0.653211  0.620976  0.562517  0.629953  0.681467  0.717956  \n",
      "chettan  0.684621  0.579749  0.544774  0.456133  0.558116  0.596601  0.581448  \n",
      "chettan  0.595641  0.559973  0.507564  0.502300  0.541869  0.540973  0.530982  \n",
      "chettan  0.601479  0.562913  0.508693  0.538208  0.541751  0.599481  0.578200  \n",
      "chettan  0.641961  0.619295  0.517767  0.563242  0.579312  0.639134  0.601648  \n",
      "chettan  0.689826  0.580418  0.516861  0.524843  0.529911  0.566635  0.566890  \n",
      "maria    0.633724  0.613324  0.603061  0.597333  0.584443  0.614297  0.616703  \n",
      "maria    0.659060  0.732314  0.711924  0.603153  0.680370  0.763351  0.740925  \n",
      "maria    0.653669  0.651927  0.660822  0.581880  0.631783  0.681734  0.669565  \n",
      "maria    0.673937  0.668702  0.637565  0.545849  0.633799  0.692177  0.660604  \n",
      "maria    0.647038  0.630861  0.604699  0.605168  0.595427  0.632386  0.631395  \n",
      "maria    0.696091  0.682400  0.637636  0.535463  0.640152  0.693003  0.671309  \n",
      "maria    0.625515  0.664016  0.653832  0.629059  0.644209  0.692798  0.692156  \n",
      "maria    0.644440  0.661390  0.683438  0.554893  0.663890  0.711452  0.695439  \n",
      "miffi    0.645441  0.636902  0.702351  0.523271  0.698093  0.688847  0.688888  \n",
      "miffi    0.656723  0.614022  0.681082  0.510043  0.686784  0.684772  0.678883  \n",
      "miffi    0.657021  0.663699  0.686509  0.588801  0.681916  0.662461  0.708664  \n",
      "par_ma   0.765124  0.600785  0.598265  0.642332  0.627394  0.662506  0.642547  \n",
      "par_ma   0.817696  0.657099  0.602580  0.712907  0.621790  0.671283  0.663317  \n",
      "par_ma   1.000000  0.713066  0.700862  0.696619  0.687376  0.696993  0.727332  \n",
      "parvati  0.713066  1.000000  0.885264  0.833683  0.907527  0.883038  0.941956  \n",
      "parvati  0.700862  0.885264  1.000000  0.805642  0.937173  0.868241  0.922593  \n",
      "parvati  0.696619  0.833683  0.805642  1.000000  0.818260  0.787203  0.811805  \n",
      "parvati  0.687376  0.907527  0.937173  0.818260  1.000000  0.903396  0.925234  \n",
      "parvati  0.696993  0.883038  0.868241  0.787203  0.903396  1.000000  0.919213  \n",
      "parvati  0.727332  0.941956  0.922593  0.811805  0.925234  0.919213  1.000000  \n",
      "\n",
      "[28 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define a dataset and data loader\n",
    "dataset = datasets.ImageFolder('data/pic')\n",
    "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\n",
    "loader = DataLoader(dataset, collate_fn=lambda x: x[0])\n",
    "\n",
    "# Perfom MTCNN facial detection\n",
    "aligned = []\n",
    "names = []\n",
    "for x, y in loader:\n",
    "    x_aligned, prob = mtcnn(x, return_prob=True)\n",
    "    if x_aligned is not None:\n",
    "        print('Face detected with probability: {:8f}'.format(prob))\n",
    "        aligned.append(x_aligned)\n",
    "        names.append(dataset.idx_to_class[y])\n",
    "\n",
    "# Calculate image embeddings\n",
    "aligned = torch.stack(aligned).to(device)\n",
    "embeddings = resnet(aligned).cpu()\n",
    "\n",
    "\n",
    "\n",
    "# Print distance matrix for classes\n",
    "\n",
    "cos_sim = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "for i in range(0,len(names)):\n",
    "    emb=embeddings[i].unsqueeze(0) \n",
    "    dist =cos(embeddings[0],emb)  # The cosine similarity between the embeddings.\n",
    "    \n",
    "    \n",
    "dists = [[cos(e1,e2).item() for e2 in embeddings] for e1 in embeddings]\n",
    "print(pd.DataFrame(dists, columns=names, index=names)) # helpful while analysing the results and for determining the value of threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  STEP 2: Opening the WEBCAM and getting the faces recognised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essential  definitions\n",
    "#### We use haarcascade classifier for detecting faces,as it is much faster than mtcnn,since in webcam ,speed is at higher edge .Considering no occlusions,haarcascades give much overall performance than mtcnn in real life cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "mtcnn=MTCNN(keep_all=True)\n",
    "cos_sim = nn.CosineSimilarity(dim=-1, eps=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera open\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "parvati\n",
      "maria\n",
      "miffi\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "miffi\n",
      "miffi\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "miffi\n",
      "miffi\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "parvati\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "parvati\n",
      "parvati\n",
      "parvati\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "parvati\n",
      "parvati\n",
      "parvati\n",
      "parvati\n",
      "amma\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "parvati\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "parvati\n",
      "parvati\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "miffi\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "miffi\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "miffi\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "miffi\n",
      "miffi\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "par_ma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "amma\n",
      "maria\n",
      "amma\n",
      "amma\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "miffi\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n",
      "maria\n"
     ]
    }
   ],
   "source": [
    "vs = WebcamVideoStream(src=0).start()\n",
    "print(\"camera open\")\n",
    "while True:\n",
    "    im= vs.read()\n",
    "    im=cv2.flip(im,1) #Flip to act as a mirror\n",
    "    \n",
    "    try:\n",
    "        frame = imutils.resize(im, width=400)\n",
    "        faces = classifier.detectMultiScale(frame)\n",
    "        path=\"./data/testing...\".format(i)\n",
    "        if not os.path.exists('./data/testing...'.format(i)):\n",
    "            os.makedirs('./data/testing...'.format(i))\n",
    "        img_name = \"im_{}.jpg\".format(i)    \n",
    "        cv2.imwrite(os.path.join(path,img_name),frame)\n",
    "        imgName=\"./data/testing.../im_{}.jpg\".format(i)\n",
    "        \n",
    "    # Get cropped and prewhitened image tensor\n",
    "        img=Image.open(imgName)\n",
    "        i=i+1\n",
    "        img_cropped = mtcnn(img)\n",
    "        boxes,prob=mtcnn.detect(img)\n",
    "        img_draw = img.copy()\n",
    "        draw = ImageDraw.Draw(img_draw)\n",
    "        for i, box in enumerate(boxes):\n",
    "            draw.rectangle(box.tolist())\n",
    "            extract_face(img, box, save_path='detected_face_{}.png'.format(i))\n",
    "        img_draw.save('annotated_faces.png')\n",
    "        ima=cv2.imread('annotated_faces.png')\n",
    "        \n",
    "    # Calculate embedding (unsqueeze to add batch dimension)\n",
    "        img_embedding = resnet(img_cropped)\n",
    "        verify(img_embedding)\n",
    "        cv2.imshow('Detecting...',ima)\n",
    "    \n",
    "    except:\n",
    "        text=\"No image found\"\n",
    "        cv2.putText(ima, text, (((box[2]-box[0])/2).astype(int),box[3].astype(int)),cv2.FONT_HERSHEY_DUPLEX, 1, (255,255,255), 1)\n",
    "        cv2.imshow('Detecting...',ima)\n",
    "            \n",
    "    \n",
    "       \n",
    "    key = cv2.waitKey(1)\n",
    "    \n",
    "        # if Esc key is press then break out of the loop \n",
    "    if key == 27:#The Esc key\n",
    "        break         \n",
    "cv2.destroyAllWindows() \n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
