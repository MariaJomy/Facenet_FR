{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  STEP 1:Getting Embeddings.\n",
    "\n",
    "## It detects the face and returns probability with which the face is detected.\n",
    "## It returns the 512 embeddings vector,the most crucial for face recognition .\n",
    "## MTCNN is used for face detection,and for finding the bounding box,(which is later used for drawing box around the face).\n",
    "## Inception Resnet V1 model (defined here as resnet) is used for finding the embeddings vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essential imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # for files\n",
    "import random\n",
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1,extract_face   #cloned from facenet pytorch model from github into \"facenet_pytorch\"\n",
    "from PIL import Image,ImageDraw\n",
    "import torch\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essential functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cos_sim(): Returns the dot product of the vectors in the function.\n",
    "### uses numpy funcions linalg.norm() and dot()\n",
    "\n",
    "\n",
    "#### numpy.linalg.norm(x, ord=None, axis=None, keepdims=False)\n",
    "\n",
    "    Matrix or vector norm.\n",
    "\n",
    "    This function is able to return one of eight different matrix norms, or one of an infinite number of vector norms (described below), depending on the value of the ord parameter.\n",
    "    Parameters:\t\n",
    "\n",
    "    x : array_like\n",
    "\n",
    "        Input array. If axis is None, x must be 1-D or 2-D.\n",
    "    ord : {non-zero int, inf, -inf, ‘fro’, ‘nuc’}, optional\n",
    "\n",
    "        Order of the norm (see table under Notes). inf means numpy’s inf object.\n",
    "    axis : {int, 2-tuple of ints, None}, optional\n",
    "\n",
    "        If axis is an integer, it specifies the axis of x along which to compute the vector norms. If axis is a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of these matrices are computed. If axis is None then either a vector norm (when x is 1-D) or a matrix norm (when x is 2-D) is returned.\n",
    "\n",
    "    keepdims : bool, optional\n",
    "\n",
    "        If this is set to True, the axes which are normed over are left in the result as dimensions with size one. With this option the result will broadcast correctly against the original x.\n",
    "\n",
    "        \n",
    "\n",
    "    Returns:\t\n",
    "\n",
    "    n : float or ndarray\n",
    "\n",
    "        Norm of the matrix or vector(s).\n",
    "        \n",
    "        \n",
    "####   numpy.dot(a, b, out=None)¶\n",
    "\n",
    "    Dot product of two arrays. Specifically,\n",
    "\n",
    "        If both a and b are 1-D arrays, it is inner product of vectors (without complex conjugation).\n",
    "\n",
    "        If both a and b are 2-D arrays, it is matrix multiplication, but using matmul or a @ b is preferred.\n",
    "\n",
    "        If either a or b is 0-D (scalar), it is equivalent to multiply and using numpy.multiply(a, b) or a * b is preferred.\n",
    "\n",
    "        If a is an N-D array and b is a 1-D array, it is a sum product over the last axis of a and b.\n",
    "\n",
    "        If a is an N-D array and b is an M-D array (where M>=2), it is a sum product over the last axis of a and the second-to-last axis of b:\n",
    "\n",
    "        dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n",
    "\n",
    "    Parameters:\t\n",
    "\n",
    "    a : array_like\n",
    "\n",
    "        First argument.\n",
    "    b : array_like\n",
    "\n",
    "        Second argument.\n",
    "    out : ndarray, optional\n",
    "\n",
    "        Output argument. This must have the exact kind that would be returned if it was not used. In particular, it must have the right type, must be C-contiguous, and its dtype must be the dtype that would be returned for dot(a,b). This is a performance feature. Therefore, if these conditions are not met, an exception is raised, instead of attempting to be flexible.\n",
    "\n",
    "    Returns\n",
    "\n",
    "    output : ndarray\n",
    "\n",
    "        Returns the dot product of a and b. If a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. If out is given, then it is returned.\n",
    "\n",
    "    Raises:\t\n",
    "\n",
    "    ValueError\n",
    "\n",
    "        If the last dimension of a is not the same size as the second-to-last dimension of b.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    \"\"\"Takes 2 vectors a, b and returns the cosine similarity according \n",
    "    to the definition of the dot product\"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cos(): cos_sim returns real numbers,where negative numbers have different interpretations.So we use this function to return only positive values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(a,b):\n",
    "    minx = -1 \n",
    "    maxx = 1\n",
    "    return (cos_sim(a,b)- minx)/(maxx-minx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify():\n",
    "#### The key function in face recognition.\n",
    " It takes the embedding vector of the test image and compares it with the embedding vector of the existing dataset.\n",
    "The chosen threshold is 0.81(by trial and error and by checking accuracy(which is seen later)).\n",
    "If the image is not recognised,nothing is mentioned,but still if face is detected,it draws the bounding box around it.\n",
    "we print the names(also its mentioned in the image).\n",
    "\n",
    "Here,boxes is a numpy nd array,obtained from mtcnn.detect(),containing the coordinates of bounding boxes of thedetected faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(embedding):\n",
    "    found=0 \n",
    "    dis=[()]\n",
    "    for i,k in enumerate(embeddings):\n",
    "        for j,l in enumerate(embedding):\n",
    "            dist =cos(k,l)\n",
    "            if dist > 0.73:\n",
    "                d=dist.tolist()\n",
    "                dis.append((d,i,j))                 \n",
    "    if len(dis)>1:           \n",
    "        res=max(dis)\n",
    "        a=res[1]\n",
    "        text=names[a]\n",
    "       \n",
    "    else:\n",
    "        text=\"Not Identified\"\n",
    "    return text        \n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_files():\n",
    "Gets the file list an save it as prediction set.We can save a part of file list as training and a part as testing.But snce we already have the dataset for training, we directly use the entire files as prediction set(or testing set).\n",
    "\n",
    "If training and predicting are taken from files, then:\n",
    "\n",
    "  training = files[:int(len(files)*0.80)] #get first 80% of file list\n",
    "  \n",
    "  prediction = files[-int(len(files)*0.20):] #get last 20% of file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(nam): #Define function to get file list and save it as prediction dataset\n",
    "    files = glob.glob(\"data/test_public/%s/*\" %nam)\n",
    "    prediction = files[:int(len(files))] \n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make_sets():\n",
    "\n",
    "This function stores the files(here prediction dataset) and theor coresponding labels.\n",
    "\n",
    "The same can be done for training dataset(if we take training dataset from files)\n",
    "\n",
    "training_data = []\n",
    "\n",
    "training_labels = []\n",
    "\n",
    "prediction_data = []\n",
    "\n",
    "prediction_labels = []\n",
    "    \n",
    "\n",
    "    for nam in name:\n",
    "    \n",
    "        training, prediction = get_files(emotion)\n",
    "        #Append data to training and prediction list, and generate labels 0-7\n",
    "            \n",
    "            for item in training:\n",
    "                 \n",
    "                img = Image.open(item)\n",
    "                \n",
    "                img_cropped = mtcnn(img)\n",
    "                \n",
    "                training_data.append(img_cropped)\n",
    "                \n",
    "                training_labels_labels.append(nam)\n",
    "     \n",
    "            for item in prediction: #repeat above process for prediction set\n",
    "            \n",
    "                img = Image.open(item)\n",
    "            \n",
    "                img_cropped = mtcnn(img)\n",
    "                \n",
    "                prediction_data.append(img_cropped)\n",
    "                \n",
    "                prediction_labels.append(nam)\n",
    "     \n",
    "    return training_data, training_labels, prediction_data, prediction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sets():  # store files and their corresponding labels in the prediction dataset\n",
    "    prediction_data = []\n",
    "    prediction_labels = []\n",
    "    for nam in name:\n",
    "        prediction = get_files(nam)\n",
    "        for item in prediction: \n",
    "            img = Image.open(item)\n",
    "            img_cropped = mtcnn(img)\n",
    "            prediction_data.append(img_cropped)\n",
    "            prediction_labels.append(nam)\n",
    "     \n",
    "                  \n",
    "    return prediction_data, prediction_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_recogniser():\n",
    "#### The main accuaracy checking function.\n",
    "If it is correctly identified,'correct' and 'count' is incremented.Else 'incorrect' and 'count'(cnt) is incremented.\n",
    "\n",
    "Accuracy is defined as percentage of correct as againt the total number of images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_recognizer():\n",
    "    prediction_data, prediction_labels = make_sets()\n",
    "    print (\"predicting classification set\")\n",
    "    cnt = 0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    none=0\n",
    "    for image in prediction_data:\n",
    "        im=resnet(image)\n",
    "        pred=verify(im)\n",
    "        if pred == prediction_labels[cnt]:\n",
    "            correct += 1\n",
    "            cnt += 1\n",
    "        else:\n",
    "            print('correct:',prediction_labels[cnt],'predicted:',pred)\n",
    "            incorrect += 1\n",
    "            cnt += 1\n",
    "    print(correct,incorrect,correct+incorrect)       \n",
    "    return ((100*correct)/(correct + incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essential definitions.\n",
    "#### name contains the names of the folders of the test images in prediction dataset.\n",
    "#### Since we use a pretrained model(in VGGFACE2 database),we require only cpu.So we define the device to be 'cpu'.\n",
    "#### The pretrained model Inception Resnet V1 is called as resnet.\n",
    "#### An instance of class MTCNN (as mtcnn) ,the parameters can be changed (refer facenet_pytorch repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "name= ['Mammooty','Mohanlal','ShahRukh','Shobana','SriDevi','Deepika Padukone','Nazriya','Varun Dhawan','Hritik Roshan','Nivin Pauly']  #names of the folders in ur prediction dataset\n",
    "\n",
    "device = torch.device('cpu')        \n",
    "\n",
    "print('Running on device: {}'.format(device))\n",
    "\n",
    "# Define MTCNN module\n",
    "\n",
    "# Note that, since MTCNN is a collection of neural nets and other code, the\n",
    "# device must be passed in the following way to enable copying of objects when\n",
    "# needed internally.\n",
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, prewhiten=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "\n",
    "# Define Inception Resnet V1 module\n",
    "\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detected with probability: 0.999736\n",
      "Face detected with probability: 0.999283\n",
      "Face detected with probability: 0.999999\n",
      "Face detected with probability: 0.999980\n",
      "Face detected with probability: 0.999376\n",
      "Face detected with probability: 0.998882\n",
      "Face detected with probability: 0.999959\n",
      "Face detected with probability: 0.999958\n",
      "Face detected with probability: 0.999897\n",
      "Face detected with probability: 0.999304\n",
      "Face detected with probability: 0.999617\n",
      "Face detected with probability: 0.999927\n",
      "Face detected with probability: 0.999963\n",
      "Face detected with probability: 0.999512\n",
      "Face detected with probability: 0.999995\n",
      "Face detected with probability: 0.999994\n",
      "Face detected with probability: 0.999873\n",
      "Face detected with probability: 0.999963\n",
      "Face detected with probability: 0.999880\n",
      "Face detected with probability: 0.999899\n",
      "Face detected with probability: 0.999999\n",
      "Face detected with probability: 0.999996\n",
      "Face detected with probability: 0.999983\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 0.999950\n",
      "Face detected with probability: 0.999954\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 0.999994\n",
      "Face detected with probability: 0.999999\n",
      "Face detected with probability: 0.999997\n",
      "Face detected with probability: 0.999999\n",
      "Face detected with probability: 0.990267\n",
      "Face detected with probability: 0.999995\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 0.999993\n",
      "Face detected with probability: 0.999950\n",
      "Face detected with probability: 0.999954\n",
      "Face detected with probability: 0.999908\n",
      "Face detected with probability: 0.999984\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 0.999960\n",
      "Face detected with probability: 0.994620\n",
      "Face detected with probability: 0.999808\n",
      "Face detected with probability: 0.999963\n",
      "Face detected with probability: 0.999996\n",
      "Face detected with probability: 0.999959\n",
      "Face detected with probability: 0.999949\n",
      "Face detected with probability: 0.999917\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 0.999992\n",
      "Face detected with probability: 0.998136\n",
      "Face detected with probability: 0.999930\n",
      "Face detected with probability: 0.999945\n",
      "Face detected with probability: 0.998886\n",
      "Face detected with probability: 0.999961\n",
      "Face detected with probability: 0.997926\n",
      "                  Deepika Padukone  Deepika Padukone  Deepika Padukone  \\\n",
      "Deepika Padukone          1.000000          0.854529          0.821014   \n",
      "Deepika Padukone          0.854529          1.000000          0.849726   \n",
      "Deepika Padukone          0.821014          0.849726          1.000000   \n",
      "Deepika Padukone          0.777288          0.867032          0.863863   \n",
      "Deepika Padukone          0.853608          0.851109          0.849691   \n",
      "Deepika Padukone          0.809841          0.880209          0.894222   \n",
      "Hritik Roshan             0.678676          0.684578          0.661934   \n",
      "Hritik Roshan             0.630765          0.611940          0.616403   \n",
      "Hritik Roshan             0.631530          0.612358          0.614137   \n",
      "Hritik Roshan             0.634693          0.635103          0.583675   \n",
      "Hritik Roshan             0.653154          0.632224          0.605835   \n",
      "Mammooty                  0.588453          0.609790          0.610484   \n",
      "Mammooty                  0.584156          0.545643          0.552782   \n",
      "Mammooty                  0.568863          0.501550          0.475846   \n",
      "Mammooty                  0.691765          0.615492          0.658294   \n",
      "Mammooty                  0.609477          0.618880          0.597335   \n",
      "Mammooty                  0.531534          0.503818          0.471829   \n",
      "Mohanlal                  0.555536          0.564205          0.564682   \n",
      "Mohanlal                  0.550534          0.536262          0.453326   \n",
      "Mohanlal                  0.553338          0.571496          0.592554   \n",
      "Mohanlal                  0.516286          0.521646          0.506350   \n",
      "Mohanlal                  0.544604          0.604522          0.537299   \n",
      "Mohanlal                  0.517340          0.533078          0.537776   \n",
      "Mohanlal                  0.594638          0.514832          0.526190   \n",
      "Nazriya                   0.645347          0.562700          0.616262   \n",
      "Nazriya                   0.758005          0.686895          0.690901   \n",
      "Nazriya                   0.709910          0.717469          0.706389   \n",
      "Nazriya                   0.682659          0.635924          0.642968   \n",
      "Nazriya                   0.604780          0.586938          0.557914   \n",
      "Nivin Pauly               0.523004          0.431769          0.509266   \n",
      "Nivin Pauly               0.549824          0.458014          0.531460   \n",
      "Nivin Pauly               0.664476          0.626727          0.660468   \n",
      "Nivin Pauly               0.507656          0.406488          0.483902   \n",
      "Nivin Pauly               0.620538          0.555766          0.646648   \n",
      "ShahRukh                  0.613828          0.534717          0.556930   \n",
      "ShahRukh                  0.554718          0.526104          0.554076   \n",
      "ShahRukh                  0.598437          0.597923          0.585408   \n",
      "ShahRukh                  0.554707          0.554755          0.578700   \n",
      "ShahRukh                  0.591869          0.565693          0.549832   \n",
      "Shobana                   0.609739          0.574711          0.634587   \n",
      "Shobana                   0.650114          0.638025          0.680383   \n",
      "Shobana                   0.531933          0.501213          0.648172   \n",
      "Shobana                   0.670764          0.628964          0.665553   \n",
      "Shobana                   0.715510          0.700552          0.653208   \n",
      "Shobana                   0.704989          0.645076          0.647580   \n",
      "Shobana                   0.727853          0.613396          0.675426   \n",
      "SriDevi                   0.695476          0.669473          0.577285   \n",
      "SriDevi                   0.747869          0.724724          0.642165   \n",
      "SriDevi                   0.694837          0.709248          0.647762   \n",
      "SriDevi                   0.696447          0.704837          0.561755   \n",
      "SriDevi                   0.681079          0.674619          0.613069   \n",
      "SriDevi                   0.654243          0.650028          0.602665   \n",
      "Varun Dhawan              0.580927          0.550359          0.515504   \n",
      "Varun Dhawan              0.581857          0.609316          0.505748   \n",
      "Varun Dhawan              0.563253          0.569049          0.522751   \n",
      "Varun Dhawan              0.535719          0.555373          0.529801   \n",
      "Varun Dhawan              0.568423          0.579862          0.517720   \n",
      "\n",
      "                  Deepika Padukone  Deepika Padukone  Deepika Padukone  \\\n",
      "Deepika Padukone          0.777288          0.853608          0.809841   \n",
      "Deepika Padukone          0.867032          0.851109          0.880209   \n",
      "Deepika Padukone          0.863863          0.849691          0.894222   \n",
      "Deepika Padukone          1.000000          0.806864          0.872613   \n",
      "Deepika Padukone          0.806864          1.000000          0.868245   \n",
      "Deepika Padukone          0.872613          0.868245          1.000000   \n",
      "Hritik Roshan             0.630853          0.609866          0.682065   \n",
      "Hritik Roshan             0.600600          0.530085          0.593468   \n",
      "Hritik Roshan             0.577561          0.578367          0.608398   \n",
      "Hritik Roshan             0.564800          0.572304          0.555908   \n",
      "Hritik Roshan             0.600423          0.551937          0.562323   \n",
      "Mammooty                  0.551255          0.579439          0.632887   \n",
      "Mammooty                  0.500367          0.562705          0.553163   \n",
      "Mammooty                  0.479434          0.492839          0.497847   \n",
      "Mammooty                  0.627863          0.635735          0.641752   \n",
      "Mammooty                  0.586336          0.661075          0.611486   \n",
      "Mammooty                  0.481849          0.507325          0.530319   \n",
      "Mohanlal                  0.486393          0.537139          0.582343   \n",
      "Mohanlal                  0.449488          0.464402          0.522018   \n",
      "Mohanlal                  0.508205          0.549827          0.566454   \n",
      "Mohanlal                  0.451287          0.483753          0.516893   \n",
      "Mohanlal                  0.530813          0.558836          0.539775   \n",
      "Mohanlal                  0.472535          0.526461          0.518516   \n",
      "Mohanlal                  0.503982          0.543790          0.445452   \n",
      "Nazriya                   0.627842          0.633278          0.618133   \n",
      "Nazriya                   0.729728          0.705779          0.698407   \n",
      "Nazriya                   0.776897          0.713513          0.711093   \n",
      "Nazriya                   0.726024          0.653038          0.657249   \n",
      "Nazriya                   0.651278          0.604221          0.638470   \n",
      "Nivin Pauly               0.518636          0.448850          0.517850   \n",
      "Nivin Pauly               0.493192          0.498168          0.532099   \n",
      "Nivin Pauly               0.624158          0.625640          0.666587   \n",
      "Nivin Pauly               0.487516          0.420486          0.503314   \n",
      "Nivin Pauly               0.615623          0.585867          0.600015   \n",
      "ShahRukh                  0.474349          0.647381          0.522951   \n",
      "ShahRukh                  0.532897          0.605589          0.506410   \n",
      "ShahRukh                  0.590013          0.646097          0.585228   \n",
      "ShahRukh                  0.554953          0.589985          0.580604   \n",
      "ShahRukh                  0.515182          0.632674          0.571150   \n",
      "Shobana                   0.654059          0.578072          0.625860   \n",
      "Shobana                   0.719590          0.601492          0.672353   \n",
      "Shobana                   0.472308          0.577046          0.541510   \n",
      "Shobana                   0.703387          0.655804          0.655170   \n",
      "Shobana                   0.733700          0.642690          0.675758   \n",
      "Shobana                   0.710700          0.616345          0.643260   \n",
      "Shobana                   0.683454          0.718266          0.676959   \n",
      "SriDevi                   0.684385          0.591212          0.650338   \n",
      "SriDevi                   0.691087          0.654899          0.694225   \n",
      "SriDevi                   0.741970          0.639766          0.677401   \n",
      "SriDevi                   0.679589          0.599600          0.623115   \n",
      "SriDevi                   0.689441          0.618655          0.676372   \n",
      "SriDevi                   0.701532          0.636543          0.661157   \n",
      "Varun Dhawan              0.566140          0.593179          0.543816   \n",
      "Varun Dhawan              0.593173          0.582640          0.575966   \n",
      "Varun Dhawan              0.597301          0.622373          0.560340   \n",
      "Varun Dhawan              0.577205          0.605157          0.575476   \n",
      "Varun Dhawan              0.510183          0.602157          0.541487   \n",
      "\n",
      "                  Hritik Roshan  Hritik Roshan  Hritik Roshan  Hritik Roshan  \\\n",
      "Deepika Padukone       0.678676       0.630765       0.631530       0.634693   \n",
      "Deepika Padukone       0.684578       0.611940       0.612358       0.635103   \n",
      "Deepika Padukone       0.661934       0.616403       0.614137       0.583675   \n",
      "Deepika Padukone       0.630853       0.600600       0.577561       0.564800   \n",
      "Deepika Padukone       0.609866       0.530085       0.578367       0.572304   \n",
      "Deepika Padukone       0.682065       0.593468       0.608398       0.555908   \n",
      "Hritik Roshan          1.000000       0.845807       0.831824       0.833495   \n",
      "Hritik Roshan          0.845807       1.000000       0.907413       0.902330   \n",
      "Hritik Roshan          0.831824       0.907413       1.000000       0.866753   \n",
      "Hritik Roshan          0.833495       0.902330       0.866753       1.000000   \n",
      "Hritik Roshan          0.793114       0.880061       0.820208       0.901961   \n",
      "Mammooty               0.545187       0.715201       0.689731       0.667998   \n",
      "Mammooty               0.481846       0.615984       0.611129       0.617870   \n",
      "Mammooty               0.474605       0.673249       0.640289       0.651406   \n",
      "Mammooty               0.529371       0.634721       0.618745       0.585748   \n",
      "Mammooty               0.582852       0.618684       0.666006       0.636235   \n",
      "Mammooty               0.525346       0.600975       0.607189       0.571183   \n",
      "Mohanlal               0.597197       0.658092       0.643181       0.584149   \n",
      "Mohanlal               0.591832       0.649532       0.688380       0.583843   \n",
      "Mohanlal               0.606659       0.652020       0.614752       0.564741   \n",
      "Mohanlal               0.644546       0.672821       0.714661       0.611856   \n",
      "Mohanlal               0.610122       0.671030       0.660920       0.618610   \n",
      "Mohanlal               0.563865       0.656314       0.608549       0.598593   \n",
      "Mohanlal               0.549011       0.574233       0.549778       0.516694   \n",
      "Nazriya                0.515854       0.458896       0.465420       0.417897   \n",
      "Nazriya                0.637184       0.565277       0.585562       0.578963   \n",
      "Nazriya                0.576554       0.567498       0.571229       0.516336   \n",
      "Nazriya                0.587650       0.558181       0.593842       0.547375   \n",
      "Nazriya                0.622766       0.648468       0.572623       0.616012   \n",
      "Nivin Pauly            0.606360       0.703583       0.604081       0.650338   \n",
      "Nivin Pauly            0.579638       0.744731       0.721284       0.678976   \n",
      "Nivin Pauly            0.540187       0.550763       0.543706       0.504692   \n",
      "Nivin Pauly            0.538420       0.662184       0.583651       0.652537   \n",
      "Nivin Pauly            0.625984       0.730585       0.695330       0.701339   \n",
      "ShahRukh               0.592680       0.542965       0.579134       0.536574   \n",
      "ShahRukh               0.586325       0.573798       0.583116       0.543847   \n",
      "ShahRukh               0.572236       0.555164       0.564972       0.528482   \n",
      "ShahRukh               0.630816       0.631438       0.693982       0.589984   \n",
      "ShahRukh               0.511102       0.476554       0.542649       0.462165   \n",
      "Shobana                0.438945       0.431405       0.500867       0.419727   \n",
      "Shobana                0.572987       0.556407       0.575727       0.522507   \n",
      "Shobana                0.483472       0.508325       0.490505       0.466556   \n",
      "Shobana                0.533795       0.502346       0.537420       0.505166   \n",
      "Shobana                0.533933       0.524983       0.523073       0.508788   \n",
      "Shobana                0.506057       0.502065       0.532131       0.481892   \n",
      "Shobana                0.559708       0.469413       0.502735       0.460308   \n",
      "SriDevi                0.491795       0.507022       0.537352       0.510309   \n",
      "SriDevi                0.608237       0.572147       0.624151       0.587459   \n",
      "SriDevi                0.555420       0.550582       0.601989       0.541595   \n",
      "SriDevi                0.512491       0.522610       0.552740       0.543305   \n",
      "SriDevi                0.560223       0.523706       0.577113       0.496358   \n",
      "SriDevi                0.514724       0.522823       0.547432       0.545443   \n",
      "Varun Dhawan           0.586583       0.583013       0.564154       0.599210   \n",
      "Varun Dhawan           0.530857       0.629748       0.652034       0.653180   \n",
      "Varun Dhawan           0.509145       0.525467       0.503219       0.526769   \n",
      "Varun Dhawan           0.607926       0.654178       0.638467       0.691951   \n",
      "Varun Dhawan           0.606874       0.638987       0.628030       0.679111   \n",
      "\n",
      "                  ...   SriDevi   SriDevi   SriDevi   SriDevi   SriDevi  \\\n",
      "Deepika Padukone  ...  0.747869  0.694837  0.696447  0.681079  0.654243   \n",
      "Deepika Padukone  ...  0.724724  0.709248  0.704837  0.674619  0.650028   \n",
      "Deepika Padukone  ...  0.642165  0.647762  0.561755  0.613069  0.602665   \n",
      "Deepika Padukone  ...  0.691087  0.741970  0.679589  0.689441  0.701532   \n",
      "Deepika Padukone  ...  0.654899  0.639766  0.599600  0.618655  0.636543   \n",
      "Deepika Padukone  ...  0.694225  0.677401  0.623115  0.676372  0.661157   \n",
      "Hritik Roshan     ...  0.608237  0.555420  0.512491  0.560223  0.514724   \n",
      "Hritik Roshan     ...  0.572147  0.550582  0.522610  0.523706  0.522823   \n",
      "Hritik Roshan     ...  0.624151  0.601989  0.552740  0.577113  0.547432   \n",
      "Hritik Roshan     ...  0.587459  0.541595  0.543305  0.496358  0.545443   \n",
      "Hritik Roshan     ...  0.597759  0.559713  0.556437  0.515021  0.501220   \n",
      "Mammooty          ...  0.575703  0.529114  0.507958  0.489571  0.509199   \n",
      "Mammooty          ...  0.546604  0.483257  0.487485  0.461771  0.487822   \n",
      "Mammooty          ...  0.503832  0.458600  0.502725  0.428612  0.481745   \n",
      "Mammooty          ...  0.608206  0.544414  0.493757  0.516086  0.523735   \n",
      "Mammooty          ...  0.517754  0.488147  0.472662  0.477017  0.449869   \n",
      "Mammooty          ...  0.583393  0.464991  0.482624  0.538291  0.463889   \n",
      "Mohanlal          ...  0.453451  0.388276  0.415641  0.481999  0.395234   \n",
      "Mohanlal          ...  0.504262  0.409227  0.461023  0.492051  0.413898   \n",
      "Mohanlal          ...  0.485410  0.413784  0.425199  0.500014  0.390981   \n",
      "Mohanlal          ...  0.490310  0.451639  0.436689  0.517909  0.429124   \n",
      "Mohanlal          ...  0.471399  0.463193  0.474435  0.522343  0.450677   \n",
      "Mohanlal          ...  0.469419  0.403387  0.425285  0.483730  0.451074   \n",
      "Mohanlal          ...  0.438542  0.461230  0.401858  0.457850  0.411253   \n",
      "Nazriya           ...  0.571557  0.564317  0.531334  0.622714  0.634367   \n",
      "Nazriya           ...  0.630660  0.631730  0.578534  0.642143  0.606247   \n",
      "Nazriya           ...  0.642375  0.676964  0.633622  0.674409  0.688897   \n",
      "Nazriya           ...  0.618899  0.650240  0.617455  0.671645  0.651134   \n",
      "Nazriya           ...  0.611936  0.634917  0.609845  0.634332  0.732978   \n",
      "Nivin Pauly       ...  0.583289  0.479193  0.518184  0.491195  0.533742   \n",
      "Nivin Pauly       ...  0.560975  0.466120  0.455830  0.441308  0.501361   \n",
      "Nivin Pauly       ...  0.646504  0.585674  0.621357  0.648952  0.630790   \n",
      "Nivin Pauly       ...  0.538835  0.488892  0.520759  0.501578  0.552184   \n",
      "Nivin Pauly       ...  0.614580  0.508089  0.511723  0.534035  0.538815   \n",
      "ShahRukh          ...  0.505768  0.452992  0.403121  0.499379  0.476033   \n",
      "ShahRukh          ...  0.498877  0.464252  0.395713  0.482815  0.509642   \n",
      "ShahRukh          ...  0.561997  0.478710  0.432470  0.536167  0.500412   \n",
      "ShahRukh          ...  0.520044  0.437958  0.412286  0.462425  0.481717   \n",
      "ShahRukh          ...  0.619615  0.577038  0.516366  0.618459  0.611193   \n",
      "Shobana           ...  0.763767  0.755230  0.746385  0.804470  0.728226   \n",
      "Shobana           ...  0.724751  0.719236  0.702118  0.752229  0.738132   \n",
      "Shobana           ...  0.462269  0.457450  0.446534  0.451644  0.424206   \n",
      "Shobana           ...  0.685896  0.674046  0.672908  0.716729  0.697426   \n",
      "Shobana           ...  0.793528  0.791859  0.820273  0.852381  0.767164   \n",
      "Shobana           ...  0.770400  0.777994  0.781899  0.786397  0.802536   \n",
      "Shobana           ...  0.698397  0.643602  0.647438  0.688932  0.704482   \n",
      "SriDevi           ...  0.920984  0.882796  0.938523  0.884100  0.874126   \n",
      "SriDevi           ...  1.000000  0.885647  0.912516  0.880627  0.852426   \n",
      "SriDevi           ...  0.885647  1.000000  0.880620  0.913680  0.904562   \n",
      "SriDevi           ...  0.912516  0.880620  1.000000  0.876360  0.830622   \n",
      "SriDevi           ...  0.880627  0.913680  0.876360  1.000000  0.850036   \n",
      "SriDevi           ...  0.852426  0.904562  0.830622  0.850036  1.000000   \n",
      "Varun Dhawan      ...  0.581073  0.546686  0.509127  0.533373  0.587812   \n",
      "Varun Dhawan      ...  0.563295  0.567367  0.553466  0.507122  0.612745   \n",
      "Varun Dhawan      ...  0.530956  0.516222  0.499716  0.493197  0.578975   \n",
      "Varun Dhawan      ...  0.518875  0.446646  0.456213  0.433431  0.560730   \n",
      "Varun Dhawan      ...  0.471180  0.454287  0.481213  0.465782  0.528430   \n",
      "\n",
      "                  Varun Dhawan  Varun Dhawan  Varun Dhawan  Varun Dhawan  \\\n",
      "Deepika Padukone      0.580927      0.581857      0.563253      0.535719   \n",
      "Deepika Padukone      0.550359      0.609316      0.569049      0.555373   \n",
      "Deepika Padukone      0.515504      0.505748      0.522751      0.529801   \n",
      "Deepika Padukone      0.566140      0.593173      0.597301      0.577205   \n",
      "Deepika Padukone      0.593179      0.582640      0.622373      0.605157   \n",
      "Deepika Padukone      0.543816      0.575966      0.560340      0.575476   \n",
      "Hritik Roshan         0.586583      0.530857      0.509145      0.607926   \n",
      "Hritik Roshan         0.583013      0.629748      0.525467      0.654178   \n",
      "Hritik Roshan         0.564154      0.652034      0.503219      0.638467   \n",
      "Hritik Roshan         0.599210      0.653180      0.526769      0.691951   \n",
      "Hritik Roshan         0.553805      0.618130      0.533047      0.627788   \n",
      "Mammooty              0.572033      0.626620      0.478783      0.596120   \n",
      "Mammooty              0.579989      0.608809      0.469741      0.560331   \n",
      "Mammooty              0.527673      0.612612      0.423490      0.547456   \n",
      "Mammooty              0.682321      0.694569      0.565919      0.647762   \n",
      "Mammooty              0.477050      0.600829      0.510622      0.535810   \n",
      "Mammooty              0.590959      0.629799      0.511379      0.597737   \n",
      "Mohanlal              0.461554      0.512030      0.384258      0.480472   \n",
      "Mohanlal              0.504550      0.585774      0.491136      0.527118   \n",
      "Mohanlal              0.437531      0.396847      0.380589      0.413352   \n",
      "Mohanlal              0.434044      0.489562      0.440216      0.453822   \n",
      "Mohanlal              0.433119      0.539013      0.420253      0.469251   \n",
      "Mohanlal              0.554505      0.537217      0.523219      0.551467   \n",
      "Mohanlal              0.467150      0.539716      0.549736      0.442809   \n",
      "Nazriya               0.487864      0.450747      0.500411      0.471356   \n",
      "Nazriya               0.548501      0.507762      0.548432      0.523034   \n",
      "Nazriya               0.553733      0.551802      0.574959      0.569292   \n",
      "Nazriya               0.511116      0.522372      0.527000      0.531085   \n",
      "Nazriya               0.537272      0.549452      0.540810      0.572299   \n",
      "Nivin Pauly           0.688636      0.608597      0.576549      0.676978   \n",
      "Nivin Pauly           0.651258      0.712891      0.528997      0.707195   \n",
      "Nivin Pauly           0.504849      0.521609      0.471035      0.534377   \n",
      "Nivin Pauly           0.615704      0.642461      0.629292      0.674681   \n",
      "Nivin Pauly           0.637056      0.649224      0.578377      0.693199   \n",
      "ShahRukh              0.649323      0.503884      0.532999      0.603262   \n",
      "ShahRukh              0.684971      0.499239      0.578081      0.646796   \n",
      "ShahRukh              0.631773      0.539946      0.505351      0.608392   \n",
      "ShahRukh              0.564812      0.564752      0.565938      0.642743   \n",
      "ShahRukh              0.660314      0.559320      0.570821      0.578654   \n",
      "Shobana               0.445088      0.473950      0.445292      0.416424   \n",
      "Shobana               0.492760      0.561297      0.470242      0.489593   \n",
      "Shobana               0.403336      0.422839      0.368881      0.421225   \n",
      "Shobana               0.448298      0.534797      0.441385      0.469691   \n",
      "Shobana               0.443589      0.472073      0.503994      0.400392   \n",
      "Shobana               0.461628      0.547403      0.469393      0.466363   \n",
      "Shobana               0.562013      0.533057      0.458562      0.548019   \n",
      "SriDevi               0.525798      0.591864      0.546496      0.496841   \n",
      "SriDevi               0.581073      0.563295      0.530956      0.518875   \n",
      "SriDevi               0.546686      0.567367      0.516222      0.446646   \n",
      "SriDevi               0.509127      0.553466      0.499716      0.456213   \n",
      "SriDevi               0.533373      0.507122      0.493197      0.433431   \n",
      "SriDevi               0.587812      0.612745      0.578975      0.560730   \n",
      "Varun Dhawan          1.000000      0.806658      0.752865      0.909686   \n",
      "Varun Dhawan          0.806658      1.000000      0.723114      0.877887   \n",
      "Varun Dhawan          0.752865      0.723114      1.000000      0.771332   \n",
      "Varun Dhawan          0.909686      0.877887      0.771332      1.000000   \n",
      "Varun Dhawan          0.795419      0.833073      0.781849      0.864160   \n",
      "\n",
      "                  Varun Dhawan  \n",
      "Deepika Padukone      0.568423  \n",
      "Deepika Padukone      0.579862  \n",
      "Deepika Padukone      0.517720  \n",
      "Deepika Padukone      0.510183  \n",
      "Deepika Padukone      0.602157  \n",
      "Deepika Padukone      0.541487  \n",
      "Hritik Roshan         0.606874  \n",
      "Hritik Roshan         0.638987  \n",
      "Hritik Roshan         0.628030  \n",
      "Hritik Roshan         0.679111  \n",
      "Hritik Roshan         0.626943  \n",
      "Mammooty              0.608486  \n",
      "Mammooty              0.577393  \n",
      "Mammooty              0.577378  \n",
      "Mammooty              0.589141  \n",
      "Mammooty              0.631656  \n",
      "Mammooty              0.612631  \n",
      "Mohanlal              0.479826  \n",
      "Mohanlal              0.511401  \n",
      "Mohanlal              0.427944  \n",
      "Mohanlal              0.464542  \n",
      "Mohanlal              0.558264  \n",
      "Mohanlal              0.579628  \n",
      "Mohanlal              0.540418  \n",
      "Nazriya               0.456307  \n",
      "Nazriya               0.566481  \n",
      "Nazriya               0.570461  \n",
      "Nazriya               0.495922  \n",
      "Nazriya               0.566148  \n",
      "Nivin Pauly           0.582123  \n",
      "Nivin Pauly           0.640542  \n",
      "Nivin Pauly           0.525517  \n",
      "Nivin Pauly           0.668564  \n",
      "Nivin Pauly           0.663571  \n",
      "ShahRukh              0.618460  \n",
      "ShahRukh              0.577939  \n",
      "ShahRukh              0.511552  \n",
      "ShahRukh              0.555067  \n",
      "ShahRukh              0.558174  \n",
      "Shobana               0.369738  \n",
      "Shobana               0.469376  \n",
      "Shobana               0.389561  \n",
      "Shobana               0.418744  \n",
      "Shobana               0.391431  \n",
      "Shobana               0.454124  \n",
      "Shobana               0.481942  \n",
      "SriDevi               0.492480  \n",
      "SriDevi               0.471180  \n",
      "SriDevi               0.454287  \n",
      "SriDevi               0.481213  \n",
      "SriDevi               0.465782  \n",
      "SriDevi               0.528430  \n",
      "Varun Dhawan          0.795419  \n",
      "Varun Dhawan          0.833073  \n",
      "Varun Dhawan          0.781849  \n",
      "Varun Dhawan          0.864160  \n",
      "Varun Dhawan          1.000000  \n",
      "\n",
      "[57 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define a dataset and data loader\n",
    "dataset = datasets.ImageFolder('datas/pic')\n",
    "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\n",
    "loader = DataLoader(dataset, collate_fn=lambda x: x[0])\n",
    "\n",
    "# Perfom MTCNN facial detection\n",
    "aligned = []\n",
    "names = []\n",
    "for x, y in loader:\n",
    "    x_aligned, prob = mtcnn(x, return_prob=True)\n",
    "    if x_aligned is not None:\n",
    "        print('Face detected with probability: {:8f}'.format(prob))\n",
    "        aligned.append(x_aligned)\n",
    "        names.append(dataset.idx_to_class[y])\n",
    "\n",
    "# Calculate image embeddings\n",
    "aligned = torch.stack(aligned).to(device)\n",
    "embeddings = resnet(aligned).cpu()\n",
    "\n",
    "\n",
    "\n",
    "# Print distance matrix for classes\n",
    "\n",
    "cos_sim = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "for i in range(0,len(names)):\n",
    "    emb=embeddings[i].unsqueeze(0) \n",
    "    dist =cos(embeddings[0],emb)  # The cosine similarity between the embeddings.\n",
    "    \n",
    "    \n",
    "dists = [[cos(e1,e2).item() for e2 in embeddings] for e1 in embeddings]\n",
    "print(pd.DataFrame(dists, columns=names, index=names)) # helpful while analysing the results and for determining the value of threshold.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting classification set\n",
      "correct: Mammooty predicted: Nivin Pauly\n",
      "94 1 95\n",
      "Got 98.94736842105263 percent correct!\n",
      "\n",
      "\n",
      "end score: 98.94736842105263 percent correct!\n"
     ]
    }
   ],
   "source": [
    "mtcnn=MTCNN(keep_all=True)\n",
    "metascore = []\n",
    "\n",
    "correct = run_recognizer()\n",
    "print(\"Got\", correct, \"percent correct!\")\n",
    "metascore.append(correct)\n",
    "print (\"\\n\\nend score:\", np.mean(metascore), \"percent correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
